{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/joan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import Counter\n",
    "from config import *\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import unidecode\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk import bigrams\n",
    "import time\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"one.json\", \"rb\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [json.loads(str_) for str_ in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame.from_records(lines)\n",
    "df_tweets['h_field'] = df_tweets[\"entities\"].apply(lambda x: x['hashtags'])\n",
    "df_tweets['hashtags'] = df_tweets['h_field'].apply(lambda x: x[0]['text'] if x != [] else None)\n",
    "df_tweets['retweeted_status'] = df_tweets['retweeted_status'].apply(lambda x: 1 if str(x)== 'nan' else 0)\n",
    "df_tweets['id_tweet'] = df_tweets.index\n",
    "\n",
    "data = df_tweets[df_tweets['retweeted_status'] == 1][['id_tweet', 'text']]\n",
    "data['join'] = data[['id_tweet', 'text']].apply(lambda x: '{}|{}'.format(x[0],x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    if text:\n",
    "        return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    if text:\n",
    "        return re.sub(r'#\\w+ ?', '', text)\n",
    "    \n",
    "    return \"\"\n",
    "def remove_accents(text):\n",
    "    if text:\n",
    "        return unidecode.unidecode(text)\n",
    "        \n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_punctuation_marks(text):\n",
    "    if text:\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        new_text = text.translate(translator)\n",
    "        return \" \".join(new_text.split())\n",
    "        \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def text_to_lower_case(text):\n",
    "    if text:\n",
    "        return text.lower()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text:\n",
    "        # TODO: Remove emojis (tip: search for encode - decode)\n",
    "        returnString = \"\"\n",
    "        for character in text:\n",
    "            try:\n",
    "                character.encode(\"ascii\")\n",
    "                returnString += character\n",
    "            except UnicodeEncodeError:\n",
    "                returnString += ''\n",
    "        text = \" \".join(returnString.split())\n",
    "        \n",
    "    return text\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_multiple_whitespaces(text):\n",
    "    if text:\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_text_marks(text):\n",
    "    if text:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # TODO: replace *, ?, ... by spaces\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def split_text_and_numbers(text):\n",
    "    temp = re.compile(\"([a-zA-Z]+)([0-9]+)\") \n",
    "    try:\n",
    "        res = temp.match(text).groups()\n",
    "        text = \" \".join(res)\n",
    "    except:\n",
    "        text = text\n",
    "    return text\n",
    "\n",
    "def remove_alone_numbers(text):\n",
    "    if text:\n",
    "        text = ' '.join(filter(lambda word:word.replace('.','').isdigit()==False, text.split()))\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.split()\n",
    "    return \" \".join([x for x in text if x not in STOPWORDS])\n",
    "\n",
    "def stemming(text):\n",
    "    text = text.split()\n",
    "    return \" \".join([ps.stem(x) for x in text])\n",
    "\n",
    "def clean_text(text):\n",
    "    # Apply the different functions in order to clean the text\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = text_to_lower_case(text)\n",
    "    text = remove_text_marks(text)\n",
    "    text = remove_accents(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = split_text_and_numbers(text)\n",
    "    text = remove_alone_numbers(text)##\n",
    "    text = remove_multiple_whitespaces(text)\n",
    "    text = remove_punctuation_marks(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    \n",
    "    # Return\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Impleent the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    titleIndex = {} # dictionary to map page titles to page ids\n",
    "    for line in lines: # Remember, lines contain all documents, each line is a document\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "        terms = clean_text(line_arr[1]) #page_title + page_text\n",
    "\n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)                  \n",
    "                    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for line in data['join']:\n",
    "    arr.append(line)\n",
    "index = create_index(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    '''\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=clean_text(query)\n",
    "    #docs=set()\n",
    "    docs = set(np.linspace(0,len(df_tweets)-1, len(df_tweets)))\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.intersection(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "lockdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tracker is so informative. I’ve been usin...</td>\n",
       "      <td>shazzamac</td>\n",
       "      <td>Mon Feb 22 17:15:37 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136390026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now that the #PrimeMinister has revealed a roa...</td>\n",
       "      <td>PitmanBham</td>\n",
       "      <td>Mon Feb 22 17:14:19 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Full #UK Government roadmap to coming out ...</td>\n",
       "      <td>MarcelRidyard</td>\n",
       "      <td>Mon Feb 22 17:03:36 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389723...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: COVID-19 - 'This has to be the last ...</td>\n",
       "      <td>EvaSilver15</td>\n",
       "      <td>Mon Feb 22 17:08:52 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Key dates on the roadmap for easing the #COVID...</td>\n",
       "      <td>WazhmaQais</td>\n",
       "      <td>Mon Feb 22 17:25:11 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136390267...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet       username  \\\n",
       "0  This tracker is so informative. I’ve been usin...      shazzamac   \n",
       "1  Now that the #PrimeMinister has revealed a roa...     PitmanBham   \n",
       "2  The Full #UK Government roadmap to coming out ...  MarcelRidyard   \n",
       "3  BREAKING: COVID-19 - 'This has to be the last ...    EvaSilver15   \n",
       "4  Key dates on the roadmap for easing the #COVID...     WazhmaQais   \n",
       "\n",
       "                             date hashtags likes retweets  \\\n",
       "0  Mon Feb 22 17:15:37 +0000 2021       []     0        0   \n",
       "1  Mon Feb 22 17:14:19 +0000 2021       []     0        0   \n",
       "2  Mon Feb 22 17:03:36 +0000 2021       []     0        0   \n",
       "3  Mon Feb 22 17:08:52 +0000 2021       []     0        0   \n",
       "4  Mon Feb 22 17:25:11 +0000 2021       []     0        0   \n",
       "\n",
       "                                                 url  \n",
       "0  https://twitter.com/twitter/statuses/136390026...  \n",
       "1  https://twitter.com/twitter/statuses/136389993...  \n",
       "2  https://twitter.com/twitter/statuses/136389723...  \n",
       "3  https://twitter.com/twitter/statuses/136389856...  \n",
       "4  https://twitter.com/twitter/statuses/136390267...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)    \n",
    "top = 10\n",
    "\n",
    "results = pd.DataFrame(columns=['tweet', 'username', 'date', 'hashtags', 'likes', 'retweets', 'url'])\n",
    "\n",
    "count = 0\n",
    "for d_id in docs[:top] :\n",
    "    results.loc[count,'tweet'] = df_tweets.loc[d_id, \"text\"]\n",
    "    results.loc[count,'username'] = df_tweets.loc[d_id, \"user\"]['screen_name']\n",
    "    results.loc[count,'date'] = df_tweets.loc[d_id, \"created_at\"]\n",
    "    results.loc[count,'hashtags'] = [df_tweets.loc[0,'entities']['hashtags'][i]['text'] for i in range(len(df_tweets.loc[0,'entities']['hashtags']))]\n",
    "    results.loc[count,'likes'] = df_tweets.loc[d_id, \"favorite_count\"]\n",
    "    results.loc[count,'retweets'] = df_tweets.loc[d_id, \"retweet_count\"]\n",
    "    results.loc[count,'url'] = \"https://twitter.com/twitter/statuses/\"+str(df_tweets.loc[d_id, \"id\"])\n",
    "    count +=1\n",
    "    \n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(lines, numDocuments):\n",
    "\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    titleIndex=defaultdict(str)\n",
    "    idf=defaultdict(float)\n",
    "    \n",
    "    for line in lines: # Remember, lines contain all documents, each line is a document\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "        terms = clean_text(line_arr[1])         \n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]= df[term] + 1  # increment df for current term\n",
    "        \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "            # Compute idf following the formula (3) above. HINT: use np.log\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf, titleIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 12.63 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "numDocuments = len(data)\n",
    "index, tf, df, idf, titleIndex = create_index_tfidf(data['join'], numDocuments)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rankDocuments(terms, docs, index, idf, tf, titleIndex):\n",
    "\n",
    "        \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    # HINT: use when computing tf for queryVector\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[termIndex]/query_norm * idf[term] \n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine siilarity\n",
    "    # see np.dot\n",
    "    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=clean_text(query)\n",
    "    #docs=set()\n",
    "    docs = set(np.linspace(0,len(df_tweets)-1, len(df_tweets)))\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.intersection(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, titleIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: lockdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tracker is so informative. I’ve been usin...</td>\n",
       "      <td>shazzamac</td>\n",
       "      <td>Mon Feb 22 17:15:37 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136390026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now that the #PrimeMinister has revealed a roa...</td>\n",
       "      <td>PitmanBham</td>\n",
       "      <td>Mon Feb 22 17:14:19 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Full #UK Government roadmap to coming out ...</td>\n",
       "      <td>MarcelRidyard</td>\n",
       "      <td>Mon Feb 22 17:03:36 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389723...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: COVID-19 - 'This has to be the last ...</td>\n",
       "      <td>EvaSilver15</td>\n",
       "      <td>Mon Feb 22 17:08:52 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Key dates on the roadmap for easing the #COVID...</td>\n",
       "      <td>WazhmaQais</td>\n",
       "      <td>Mon Feb 22 17:25:11 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136390267...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet       username  \\\n",
       "0  This tracker is so informative. I’ve been usin...      shazzamac   \n",
       "1  Now that the #PrimeMinister has revealed a roa...     PitmanBham   \n",
       "2  The Full #UK Government roadmap to coming out ...  MarcelRidyard   \n",
       "3  BREAKING: COVID-19 - 'This has to be the last ...    EvaSilver15   \n",
       "4  Key dates on the roadmap for easing the #COVID...     WazhmaQais   \n",
       "\n",
       "                             date hashtags likes retweets  \\\n",
       "0  Mon Feb 22 17:15:37 +0000 2021       []     0        0   \n",
       "1  Mon Feb 22 17:14:19 +0000 2021       []     0        0   \n",
       "2  Mon Feb 22 17:03:36 +0000 2021       []     0        0   \n",
       "3  Mon Feb 22 17:08:52 +0000 2021       []     0        0   \n",
       "4  Mon Feb 22 17:25:11 +0000 2021       []     0        0   \n",
       "\n",
       "                                                 url  \n",
       "0  https://twitter.com/twitter/statuses/136390026...  \n",
       "1  https://twitter.com/twitter/statuses/136389993...  \n",
       "2  https://twitter.com/twitter/statuses/136389723...  \n",
       "3  https://twitter.com/twitter/statuses/136389856...  \n",
       "4  https://twitter.com/twitter/statuses/136390267...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def insert_query(query):\n",
    "    print('Query: {}'.format(query))\n",
    "\n",
    "    ranked_docs = search_tf_idf(query, index)    \n",
    "    top = 20\n",
    "\n",
    "    results = pd.DataFrame(columns=['tweet', 'username', 'date', 'hashtags', 'likes', 'retweets', 'url'])\n",
    "\n",
    "    count = 0\n",
    "    for d_id in docs[:top] :\n",
    "        results.loc[count,'tweet'] = df_tweets.loc[d_id, \"text\"]\n",
    "        results.loc[count,'username'] = df_tweets.loc[d_id, \"user\"]['screen_name']\n",
    "        results.loc[count,'date'] = df_tweets.loc[d_id, \"created_at\"]\n",
    "        results.loc[count,'hashtags'] = [df_tweets.loc[0,'entities']['hashtags'][i]['text'] for i in range(len(df_tweets.loc[0,'entities']['hashtags']))]\n",
    "        results.loc[count,'likes'] = df_tweets.loc[d_id, \"favorite_count\"]\n",
    "        results.loc[count,'retweets'] = df_tweets.loc[d_id, \"retweet_count\"]\n",
    "        results.loc[count,'url'] = \"https://twitter.com/twitter/statuses/\"+str(df_tweets.loc[d_id, \"id\"])\n",
    "        count +=1\n",
    "    return results\n",
    "    \n",
    "results = insert_query('lockdown')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster-based search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_using_new_ranking(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=clean_text(query)\n",
    "    #docs=set()\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.union(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, titleIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "lockdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This tracker is so informative. I’ve been usin...</td>\n",
       "      <td>shazzamac</td>\n",
       "      <td>Mon Feb 22 17:15:37 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136390026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now that the #PrimeMinister has revealed a roa...</td>\n",
       "      <td>PitmanBham</td>\n",
       "      <td>Mon Feb 22 17:14:19 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Full #UK Government roadmap to coming out ...</td>\n",
       "      <td>MarcelRidyard</td>\n",
       "      <td>Mon Feb 22 17:03:36 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389723...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BREAKING: COVID-19 - 'This has to be the last ...</td>\n",
       "      <td>EvaSilver15</td>\n",
       "      <td>Mon Feb 22 17:08:52 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136389856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Key dates on the roadmap for easing the #COVID...</td>\n",
       "      <td>WazhmaQais</td>\n",
       "      <td>Mon Feb 22 17:25:11 +0000 2021</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://twitter.com/twitter/statuses/136390267...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet       username  \\\n",
       "0  This tracker is so informative. I’ve been usin...      shazzamac   \n",
       "1  Now that the #PrimeMinister has revealed a roa...     PitmanBham   \n",
       "2  The Full #UK Government roadmap to coming out ...  MarcelRidyard   \n",
       "3  BREAKING: COVID-19 - 'This has to be the last ...    EvaSilver15   \n",
       "4  Key dates on the roadmap for easing the #COVID...     WazhmaQais   \n",
       "\n",
       "                             date hashtags likes retweets  \\\n",
       "0  Mon Feb 22 17:15:37 +0000 2021       []     0        0   \n",
       "1  Mon Feb 22 17:14:19 +0000 2021       []     0        0   \n",
       "2  Mon Feb 22 17:03:36 +0000 2021       []     0        0   \n",
       "3  Mon Feb 22 17:08:52 +0000 2021       []     0        0   \n",
       "4  Mon Feb 22 17:25:11 +0000 2021       []     0        0   \n",
       "\n",
       "                                                 url  \n",
       "0  https://twitter.com/twitter/statuses/136390026...  \n",
       "1  https://twitter.com/twitter/statuses/136389993...  \n",
       "2  https://twitter.com/twitter/statuses/136389723...  \n",
       "3  https://twitter.com/twitter/statuses/136389856...  \n",
       "4  https://twitter.com/twitter/statuses/136390267...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 1\n",
    "\n",
    "query = df_tweets.loc[ranked_docs[:top]]['text'].values\n",
    "ranked_docs = search_using_new_ranking(query[0], index)    \n",
    "top = 20\n",
    "\n",
    "results = pd.DataFrame(columns=['tweet', 'username', 'date', 'hashtags', 'likes', 'retweets', 'url'])\n",
    "\n",
    "count = 0\n",
    "for d_id in docs[:top] :\n",
    "    results.loc[count,'tweet'] = df_tweets.loc[d_id, \"text\"]\n",
    "    results.loc[count,'username'] = df_tweets.loc[d_id, \"user\"]['screen_name']\n",
    "    results.loc[count,'date'] = df_tweets.loc[d_id, \"created_at\"]\n",
    "    results.loc[count,'hashtags'] = [df_tweets.loc[0,'entities']['hashtags'][i]['text'] for i in range(len(df_tweets.loc[0,'entities']['hashtags']))]\n",
    "    results.loc[count,'likes'] = df_tweets.loc[d_id, \"favorite_count\"]\n",
    "    results.loc[count,'retweets'] = df_tweets.loc[d_id, \"retweet_count\"]\n",
    "    results.loc[count,'url'] = \"https://twitter.com/twitter/statuses/\"+str(df_tweets.loc[d_id, \"id\"])\n",
    "    count +=1\n",
    "    \n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
