{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/joan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import Counter\n",
    "from config import *\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import unidecode\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk import bigrams\n",
    "import time\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"one.json\", \"rb\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [json.loads(str_) for str_ in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame.from_records(lines)\n",
    "df_tweets['h_field'] = df_tweets[\"entities\"].apply(lambda x: x['hashtags'])\n",
    "df_tweets['hashtags'] = df_tweets['h_field'].apply(lambda x: x[0]['text'] if x != [] else None)\n",
    "df_tweets['retweeted_status'] = df_tweets['retweeted_status'].apply(lambda x: 1 if str(x)== 'nan' else 0)\n",
    "df_tweets['id_tweet'] = df_tweets.index\n",
    "\n",
    "data = df_tweets[df_tweets['retweeted_status'] == 1][['id_tweet', 'text']]\n",
    "#data['id_tweet'] = str(data['id_tweet'])\n",
    "data['join'] = data[['id_tweet', 'text']].apply(lambda x: '{}|{}'.format(x[0],x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    if text:\n",
    "        return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    if text:\n",
    "        return re.sub(r'#\\w+ ?', '', text)\n",
    "    \n",
    "    return \"\"\n",
    "def remove_accents(text):\n",
    "    if text:\n",
    "        return unidecode.unidecode(text)\n",
    "        \n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_punctuation_marks(text):\n",
    "    if text:\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        new_text = text.translate(translator)\n",
    "        return \" \".join(new_text.split())\n",
    "        \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def text_to_lower_case(text):\n",
    "    if text:\n",
    "        return text.lower()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text:\n",
    "        # TODO: Remove emojis (tip: search for encode - decode)\n",
    "        returnString = \"\"\n",
    "        for character in text:\n",
    "            try:\n",
    "                character.encode(\"ascii\")\n",
    "                returnString += character\n",
    "            except UnicodeEncodeError:\n",
    "                returnString += ''\n",
    "        text = \" \".join(returnString.split())\n",
    "        \n",
    "    return text\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_multiple_whitespaces(text):\n",
    "    if text:\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_text_marks(text):\n",
    "    if text:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # TODO: replace *, ?, ... by spaces\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def split_text_and_numbers(text):\n",
    "    temp = re.compile(\"([a-zA-Z]+)([0-9]+)\") \n",
    "    try:\n",
    "        res = temp.match(text).groups()\n",
    "        text = \" \".join(res)\n",
    "    except:\n",
    "        text = text\n",
    "    return text\n",
    "\n",
    "def remove_alone_numbers(text):\n",
    "    if text:\n",
    "        text = ' '.join(filter(lambda word:word.replace('.','').isdigit()==False, text.split()))\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.split()\n",
    "    return \" \".join([x for x in text if x not in STOPWORDS])\n",
    "\n",
    "def stemming(text):\n",
    "    text = text.split()\n",
    "    return \" \".join([ps.stem(x) for x in text])\n",
    "\n",
    "def clean_text(text):\n",
    "    # Apply the different functions in order to clean the text\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = text_to_lower_case(text)\n",
    "    text = remove_text_marks(text)\n",
    "    text = remove_accents(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = split_text_and_numbers(text)\n",
    "    #text = remove_alone_numbers(text)\n",
    "    text = remove_multiple_whitespaces(text)\n",
    "    text = remove_punctuation_marks(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    \n",
    "    # Return\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(lines):\n",
    "    \"\"\"\n",
    "    Impleent the inverted index\n",
    "    \n",
    "    Argument:\n",
    "    lines -- collection of Wikipedia articles\n",
    "    \n",
    "    Returns:\n",
    "    index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "    list of document these keys appears in (and the positions) as values.\n",
    "    \"\"\"\n",
    "    index=defaultdict(list) \n",
    "    titleIndex = {} # dictionary to map page titles to page ids\n",
    "    for line in lines: # Remember, lines contain all documents, each line is a document\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "        terms = clean_text(line_arr[1]) #page_title + page_text\n",
    "  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "        \n",
    "        ## ===============================================================        \n",
    "        ## create the index for the current doc and store it in termdictPage\n",
    "        ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "        \n",
    "        ## Example: if the curr_doc has id 1 and his text is \n",
    "        ## \"web retrieval information retrieval\":\n",
    "        \n",
    "        ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "        \n",
    "        ## the term ‘web’ appears in document 1 in positions 0, \n",
    "        ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "        ## ===============================================================\n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "            try:\n",
    "                # if the term is already in the index for the current page (termdictPage)\n",
    "                # append the position to the corrisponding list\n",
    "                \n",
    "        ## START CODE\n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "            \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "        \n",
    "        ## END CODE                    \n",
    "                    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for line in data['join']:\n",
    "    arr.append(line)\n",
    "index = create_index(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, index):\n",
    "    '''\n",
    "    The output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=clean_text(query)\n",
    "    #docs=set()\n",
    "    docs = set(np.linspace(0,len(df_tweets)-1, len(df_tweets)))\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.intersection(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "trump\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Interior Secretary David Bernhardt, a Colorado...</td>\n",
       "      <td>{'id': 993301872751529984, 'id_str': '99330187...</td>\n",
       "      <td>Thu Dec 17 14:41:35 +0000 2020</td>\n",
       "      <td>copolitics</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90116</th>\n",
       "      <td>Trump did EVERYTHING in his power to undermine...</td>\n",
       "      <td>{'id': 828596979160797184, 'id_str': '82859697...</td>\n",
       "      <td>Thu Dec 17 15:23:12 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81929</th>\n",
       "      <td>Ivanka Trump Faces Off With Father to See How ...</td>\n",
       "      <td>{'id': 18722822, 'id_str': '18722822', 'name':...</td>\n",
       "      <td>Thu Dec 17 15:19:45 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90127</th>\n",
       "      <td>That just makes it even better . Would they ev...</td>\n",
       "      <td>{'id': 1235658301330919424, 'id_str': '1235658...</td>\n",
       "      <td>Thu Dec 17 15:23:12 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204816</th>\n",
       "      <td>Online news matters: States that voted for Tru...</td>\n",
       "      <td>{'id': 13138012, 'id_str': '13138012', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:32:33 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49138</th>\n",
       "      <td>Explains Trump's behavior...</td>\n",
       "      <td>{'id': 249739297, 'id_str': '249739297', 'name...</td>\n",
       "      <td>Thu Dec 17 15:05:39 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204788</th>\n",
       "      <td>@HMOTP @SenSchumer @realDonaldTrump I wasn't a...</td>\n",
       "      <td>{'id': 1243393335592534017, 'id_str': '1243393...</td>\n",
       "      <td>Thu Dec 17 16:32:51 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114677</th>\n",
       "      <td>@Bucshamsky @lestathavana @yashar @realDonaldT...</td>\n",
       "      <td>{'id': 910139032306356226, 'id_str': '91013903...</td>\n",
       "      <td>Thu Dec 17 15:33:51 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204791</th>\n",
       "      <td>@CNN Trump administration = anti human and not...</td>\n",
       "      <td>{'id': 526211047, 'id_str': '526211047', 'name...</td>\n",
       "      <td>Thu Dec 17 16:32:50 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40953</th>\n",
       "      <td>I forgot Trump existed LOL</td>\n",
       "      <td>{'id': 2455273345, 'id_str': '2455273345', 'na...</td>\n",
       "      <td>Thu Dec 17 15:02:09 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1402 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Interior Secretary David Bernhardt, a Colorado...   \n",
       "90116   Trump did EVERYTHING in his power to undermine...   \n",
       "81929   Ivanka Trump Faces Off With Father to See How ...   \n",
       "90127   That just makes it even better . Would they ev...   \n",
       "204816  Online news matters: States that voted for Tru...   \n",
       "...                                                   ...   \n",
       "49138                        Explains Trump's behavior...   \n",
       "204788  @HMOTP @SenSchumer @realDonaldTrump I wasn't a...   \n",
       "114677  @Bucshamsky @lestathavana @yashar @realDonaldT...   \n",
       "204791  @CNN Trump administration = anti human and not...   \n",
       "40953                          I forgot Trump existed LOL   \n",
       "\n",
       "                                                     user  \\\n",
       "0       {'id': 993301872751529984, 'id_str': '99330187...   \n",
       "90116   {'id': 828596979160797184, 'id_str': '82859697...   \n",
       "81929   {'id': 18722822, 'id_str': '18722822', 'name':...   \n",
       "90127   {'id': 1235658301330919424, 'id_str': '1235658...   \n",
       "204816  {'id': 13138012, 'id_str': '13138012', 'name':...   \n",
       "...                                                   ...   \n",
       "49138   {'id': 249739297, 'id_str': '249739297', 'name...   \n",
       "204788  {'id': 1243393335592534017, 'id_str': '1243393...   \n",
       "114677  {'id': 910139032306356226, 'id_str': '91013903...   \n",
       "204791  {'id': 526211047, 'id_str': '526211047', 'name...   \n",
       "40953   {'id': 2455273345, 'id_str': '2455273345', 'na...   \n",
       "\n",
       "                            created_at    hashtags  favorite_count  \\\n",
       "0       Thu Dec 17 14:41:35 +0000 2020  copolitics               0   \n",
       "90116   Thu Dec 17 15:23:12 +0000 2020        None               0   \n",
       "81929   Thu Dec 17 15:19:45 +0000 2020        None               0   \n",
       "90127   Thu Dec 17 15:23:12 +0000 2020        None               0   \n",
       "204816  Thu Dec 17 16:32:33 +0000 2020        None               0   \n",
       "...                                ...         ...             ...   \n",
       "49138   Thu Dec 17 15:05:39 +0000 2020        None               0   \n",
       "204788  Thu Dec 17 16:32:51 +0000 2020        None               0   \n",
       "114677  Thu Dec 17 15:33:51 +0000 2020        None               0   \n",
       "204791  Thu Dec 17 16:32:50 +0000 2020        None               0   \n",
       "40953   Thu Dec 17 15:02:09 +0000 2020        None               0   \n",
       "\n",
       "        retweet_count                                             source  \n",
       "0                   0  <a href=\"https://about.twitter.com/products/tw...  \n",
       "90116               0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "81929               0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "90127               0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "204816              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "...               ...                                                ...  \n",
       "49138               0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "204788              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "114677              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "204791              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "40953               0  <a href=\"https://about.twitter.com/products/tw...  \n",
       "\n",
       "[1402 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = search(query, index)    \n",
    "top = 10\n",
    "\n",
    "df_tweets.loc[docs][['text', 'user', 'created_at', 'hashtags' , 'favorite_count', 'retweet_count', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(lines, numDocuments):\n",
    "\n",
    "        \n",
    "    index=defaultdict(list)\n",
    "    tf=defaultdict(list) #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "    df=defaultdict(int)         #document frequencies of terms in the corpus\n",
    "    titleIndex=defaultdict(str)\n",
    "    idf=defaultdict(float)\n",
    "    \n",
    "    for line in lines: # Remember, lines contain all documents, each line is a document\n",
    "        line_arr = line.split(\"|\")\n",
    "        page_id = int(line_arr[0])\n",
    "        terms = clean_text(line_arr[1])         \n",
    "        \n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms): ## terms contains page_title + page_text\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][1].append(position)  \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[page_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "        \n",
    "        #normalize term frequencies\n",
    "        # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "        # norm is the same for all terms of a document.\n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            # posting is a list containing doc_id and the list of positions for current term in current document: \n",
    "            # posting ==> [currentdoc, [list of positions]] \n",
    "            # you can use it to inferr the frequency of current term.\n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():     \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term]= df[term] + 1  # increment df for current term\n",
    "        \n",
    "        #merge the current page index with the main index\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "            # Compute idf following the formula (3) above. HINT: use np.log\n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf, titleIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time to create the index: 40.94 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "numDocuments = len(data)\n",
    "index, tf, df, idf, titleIndex = create_index_tfidf(data['join'], numDocuments)\n",
    "print(\"Total time to create the index: {} seconds\" .format(np.round(time.time() - start_time,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rankDocuments(terms, docs, index, idf, tf, titleIndex):\n",
    "    \"\"\"\n",
    "    Perform the ranking of the results of a search based on the tf-idf weights\n",
    "    \n",
    "    Argument:\n",
    "    terms -- list of query terms\n",
    "    docs -- list of documents, to rank, matching the query\n",
    "    index -- inverted index data structure\n",
    "    idf -- inverted document frequencies\n",
    "    tf -- term frequencies\n",
    "    titleIndex -- mapping between page id and page title\n",
    "    \n",
    "    Returns:\n",
    "    Print the list of ranked documents\n",
    "    \"\"\"\n",
    "        \n",
    "    # I'm interested only on the element of the docVector corresponding to the query terms \n",
    "    # The remaing elements would became 0 when multiplied to the queryVector\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) # I call docVectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) # get the frequency of each term in the query. \n",
    "    # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "    # HINT: use when computing tf for queryVector\n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "    \n",
    "    for termIndex, term in enumerate(terms): #termIndex is the index of the term in the query\n",
    "        if term not in index:\n",
    "            continue\n",
    "                    \n",
    "        ## Compute tf*idf(normalize tf as done with documents)\n",
    "        queryVector[termIndex]=query_terms_count[termIndex]/query_norm * idf[term] \n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "            # Example of [docIndex, (doc, postings)]\n",
    "            # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "            # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "            # term is in doc 26 in positions 1,4, .....\n",
    "            # term is in doc 33 in positions 26,33, .....\n",
    "            \n",
    "            #tf[term][0] will contain the tf of the term \"term\" in the doc 26            \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term]  # TODO: check if multiply for idf\n",
    "\n",
    "    # calculate the score of each doc\n",
    "    # compute the cosine similarity between queyVector and each docVector:\n",
    "    # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine siilarity\n",
    "    # see np.dot\n",
    "    \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "    #print document titles instead if document id's\n",
    "    #resultDocs=[ titleIndex[x] for x in resultDocs ]\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    #print ('\\n'.join(resultDocs), '\\n')\n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=clean_text(query)\n",
    "    #docs=set()\n",
    "    docs = set(np.linspace(0,len(df_tweets)-1, len(df_tweets)))\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.intersection(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, titleIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "trump\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217832</th>\n",
       "      <td>They should all be jailed - Trump, Conway, Iva...</td>\n",
       "      <td>{'id': 27208882, 'id_str': '27208882', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:38:27 +0000 2020</td>\n",
       "      <td>Covid</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217682</th>\n",
       "      <td>@rambosnack @deronium @kaia198312 @realDonaldT...</td>\n",
       "      <td>{'id': 322803604, 'id_str': '322803604', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:24 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217619</th>\n",
       "      <td>Trump has tweeted about TV ratings four times ...</td>\n",
       "      <td>{'id': 16417977, 'id_str': '16417977', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:38:22 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217615</th>\n",
       "      <td>via ⁦@nytimes⁩ @cspanwj \\nTrump leaves office ...</td>\n",
       "      <td>{'id': 39504336, 'id_str': '39504336', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:38:22 +0000 2020</td>\n",
       "      <td>Cult45KILLSjobs</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217597</th>\n",
       "      <td>@LindseyGrahamSC $600 is an insult. Why haven'...</td>\n",
       "      <td>{'id': 1331645605698007045, 'id_str': '1331645...</td>\n",
       "      <td>Thu Dec 17 16:38:21 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217532</th>\n",
       "      <td>over 300,000 dead from the V, 10 inmates execu...</td>\n",
       "      <td>{'id': 1325112850818404356, 'id_str': '1325112...</td>\n",
       "      <td>Thu Dec 17 16:38:20 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216934</th>\n",
       "      <td>I like people who did NOT kiss Trump's ass.</td>\n",
       "      <td>{'id': 32655131, 'id_str': '32655131', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:38:04 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216890</th>\n",
       "      <td>If Trump would of done a Tom Cruise at the beg...</td>\n",
       "      <td>{'id': 1324681791383457793, 'id_str': '1324681...</td>\n",
       "      <td>Thu Dec 17 16:38:03 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216620</th>\n",
       "      <td>@marcorubio OMG - you are a hypocrite and a ba...</td>\n",
       "      <td>{'id': 1158829103614746625, 'id_str': '1158829...</td>\n",
       "      <td>Thu Dec 17 16:37:56 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216456</th>\n",
       "      <td>@eugenegu @realDonaldTrump But, but MSM said T...</td>\n",
       "      <td>{'id': 39924112, 'id_str': '39924112', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:37:52 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216030</th>\n",
       "      <td>@HawleyMO $600 is an insult. Why haven't you a...</td>\n",
       "      <td>{'id': 1331645605698007045, 'id_str': '1331645...</td>\n",
       "      <td>Thu Dec 17 16:37:41 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215959</th>\n",
       "      <td>@realDonaldTrump Donald Trump is the one respo...</td>\n",
       "      <td>{'id': 1326237955015368705, 'id_str': '1326237...</td>\n",
       "      <td>Thu Dec 17 16:37:40 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215897</th>\n",
       "      <td>Yes. The pandemic was made exponentially worse...</td>\n",
       "      <td>{'id': 23689569, 'id_str': '23689569', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:37:38 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215896</th>\n",
       "      <td>@realDonaldTrump \\nPresident Trump, \\nWe all k...</td>\n",
       "      <td>{'id': 3643807214, 'id_str': '3643807214', 'na...</td>\n",
       "      <td>Thu Dec 17 16:37:38 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215806</th>\n",
       "      <td>@mtgreenee The shutdowns in high COVID-19 area...</td>\n",
       "      <td>{'id': 840568195, 'id_str': '840568195', 'name...</td>\n",
       "      <td>Thu Dec 17 16:37:36 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215732</th>\n",
       "      <td>New covid deaths *yesterday* in the United Sta...</td>\n",
       "      <td>{'id': 259567469, 'id_str': '259567469', 'name...</td>\n",
       "      <td>Thu Dec 17 16:37:34 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215657</th>\n",
       "      <td>Tennessee really turned a HVAC mane into the G...</td>\n",
       "      <td>{'id': 1295913844838223874, 'id_str': '1295913...</td>\n",
       "      <td>Thu Dec 17 16:37:33 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215625</th>\n",
       "      <td>@fuckYOUimgreat @IlhanMN @SpeakerPelosi That’s...</td>\n",
       "      <td>{'id': 35991127, 'id_str': '35991127', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:37:32 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215618</th>\n",
       "      <td>Beltway Insider:  Biden, Trump, Stimulus, Chin...</td>\n",
       "      <td>{'id': 1059672216814546944, 'id_str': '1059672...</td>\n",
       "      <td>Thu Dec 17 16:37:31 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215333</th>\n",
       "      <td>Chris Hayes summed up Donald Trump’s catastrop...</td>\n",
       "      <td>{'id': 1230504509006217217, 'id_str': '1230504...</td>\n",
       "      <td>Thu Dec 17 16:37:24 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "217832  They should all be jailed - Trump, Conway, Iva...   \n",
       "217682  @rambosnack @deronium @kaia198312 @realDonaldT...   \n",
       "217619  Trump has tweeted about TV ratings four times ...   \n",
       "217615  via ⁦@nytimes⁩ @cspanwj \\nTrump leaves office ...   \n",
       "217597  @LindseyGrahamSC $600 is an insult. Why haven'...   \n",
       "217532  over 300,000 dead from the V, 10 inmates execu...   \n",
       "216934        I like people who did NOT kiss Trump's ass.   \n",
       "216890  If Trump would of done a Tom Cruise at the beg...   \n",
       "216620  @marcorubio OMG - you are a hypocrite and a ba...   \n",
       "216456  @eugenegu @realDonaldTrump But, but MSM said T...   \n",
       "216030  @HawleyMO $600 is an insult. Why haven't you a...   \n",
       "215959  @realDonaldTrump Donald Trump is the one respo...   \n",
       "215897  Yes. The pandemic was made exponentially worse...   \n",
       "215896  @realDonaldTrump \\nPresident Trump, \\nWe all k...   \n",
       "215806  @mtgreenee The shutdowns in high COVID-19 area...   \n",
       "215732  New covid deaths *yesterday* in the United Sta...   \n",
       "215657  Tennessee really turned a HVAC mane into the G...   \n",
       "215625  @fuckYOUimgreat @IlhanMN @SpeakerPelosi That’s...   \n",
       "215618  Beltway Insider:  Biden, Trump, Stimulus, Chin...   \n",
       "215333  Chris Hayes summed up Donald Trump’s catastrop...   \n",
       "\n",
       "                                                     user  \\\n",
       "217832  {'id': 27208882, 'id_str': '27208882', 'name':...   \n",
       "217682  {'id': 322803604, 'id_str': '322803604', 'name...   \n",
       "217619  {'id': 16417977, 'id_str': '16417977', 'name':...   \n",
       "217615  {'id': 39504336, 'id_str': '39504336', 'name':...   \n",
       "217597  {'id': 1331645605698007045, 'id_str': '1331645...   \n",
       "217532  {'id': 1325112850818404356, 'id_str': '1325112...   \n",
       "216934  {'id': 32655131, 'id_str': '32655131', 'name':...   \n",
       "216890  {'id': 1324681791383457793, 'id_str': '1324681...   \n",
       "216620  {'id': 1158829103614746625, 'id_str': '1158829...   \n",
       "216456  {'id': 39924112, 'id_str': '39924112', 'name':...   \n",
       "216030  {'id': 1331645605698007045, 'id_str': '1331645...   \n",
       "215959  {'id': 1326237955015368705, 'id_str': '1326237...   \n",
       "215897  {'id': 23689569, 'id_str': '23689569', 'name':...   \n",
       "215896  {'id': 3643807214, 'id_str': '3643807214', 'na...   \n",
       "215806  {'id': 840568195, 'id_str': '840568195', 'name...   \n",
       "215732  {'id': 259567469, 'id_str': '259567469', 'name...   \n",
       "215657  {'id': 1295913844838223874, 'id_str': '1295913...   \n",
       "215625  {'id': 35991127, 'id_str': '35991127', 'name':...   \n",
       "215618  {'id': 1059672216814546944, 'id_str': '1059672...   \n",
       "215333  {'id': 1230504509006217217, 'id_str': '1230504...   \n",
       "\n",
       "                            created_at         hashtags  favorite_count  \\\n",
       "217832  Thu Dec 17 16:38:27 +0000 2020            Covid               0   \n",
       "217682  Thu Dec 17 16:38:24 +0000 2020             None               0   \n",
       "217619  Thu Dec 17 16:38:22 +0000 2020             None               0   \n",
       "217615  Thu Dec 17 16:38:22 +0000 2020  Cult45KILLSjobs               0   \n",
       "217597  Thu Dec 17 16:38:21 +0000 2020             None               0   \n",
       "217532  Thu Dec 17 16:38:20 +0000 2020             None               0   \n",
       "216934  Thu Dec 17 16:38:04 +0000 2020             None               0   \n",
       "216890  Thu Dec 17 16:38:03 +0000 2020             None               0   \n",
       "216620  Thu Dec 17 16:37:56 +0000 2020             None               0   \n",
       "216456  Thu Dec 17 16:37:52 +0000 2020             None               0   \n",
       "216030  Thu Dec 17 16:37:41 +0000 2020             None               0   \n",
       "215959  Thu Dec 17 16:37:40 +0000 2020             None               0   \n",
       "215897  Thu Dec 17 16:37:38 +0000 2020             None               0   \n",
       "215896  Thu Dec 17 16:37:38 +0000 2020             None               0   \n",
       "215806  Thu Dec 17 16:37:36 +0000 2020             None               0   \n",
       "215732  Thu Dec 17 16:37:34 +0000 2020             None               0   \n",
       "215657  Thu Dec 17 16:37:33 +0000 2020             None               0   \n",
       "215625  Thu Dec 17 16:37:32 +0000 2020             None               0   \n",
       "215618  Thu Dec 17 16:37:31 +0000 2020             None               0   \n",
       "215333  Thu Dec 17 16:37:24 +0000 2020             None               0   \n",
       "\n",
       "        retweet_count                                             source  \n",
       "217832              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217682              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217619              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "217615              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217597              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217532              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "216934              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "216890              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "216620              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "216456              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "216030              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "215959              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "215897              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "215896              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "215806              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "215732              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "215657              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "215625              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "215618              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "215333              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 20\n",
    "\n",
    "df_tweets.loc[ranked_docs[:top]][['text', 'user', 'created_at', 'hashtags' , 'favorite_count', 'retweet_count', 'source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_using_new_ranking(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=clean_text(query)\n",
    "    #docs=set()\n",
    "    docs = set()\n",
    "    for term in query:\n",
    "    ## START DODE\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            # docs = docs Union termDocs\n",
    "            docs = docs.union(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf, titleIndex)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "covid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>created_at</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217856</th>\n",
       "      <td>@CarrieeeeC They're having panic attacks over ...</td>\n",
       "      <td>{'id': 23965688, 'id_str': '23965688', 'name':...</td>\n",
       "      <td>Thu Dec 17 16:38:28 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217837</th>\n",
       "      <td>What if I told you we already have the vaccine...</td>\n",
       "      <td>{'id': 972294050232532997, 'id_str': '97229405...</td>\n",
       "      <td>Thu Dec 17 16:38:27 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217836</th>\n",
       "      <td>The vaccine gonna be $600 😭</td>\n",
       "      <td>{'id': 3741790334, 'id_str': '3741790334', 'na...</td>\n",
       "      <td>Thu Dec 17 16:38:27 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217831</th>\n",
       "      <td>We deserve royalties! We paid for the vaccine ...</td>\n",
       "      <td>{'id': 1008411133458436096, 'id_str': '1008411...</td>\n",
       "      <td>Thu Dec 17 16:38:27 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217794</th>\n",
       "      <td>@realDonaldTrump People are dying because of t...</td>\n",
       "      <td>{'id': 1095880612634918912, 'id_str': '1095880...</td>\n",
       "      <td>Thu Dec 17 16:38:26 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217783</th>\n",
       "      <td>Some vaccine doses kept too cold, Pfizer havin...</td>\n",
       "      <td>{'id': 360024367, 'id_str': '360024367', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:26 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://publicize.wp.com/\" rel=\"nofoll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217770</th>\n",
       "      <td>Give me an update on the H1N1 VACCINE. Stand u...</td>\n",
       "      <td>{'id': 278770541, 'id_str': '278770541', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:25 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217769</th>\n",
       "      <td>@NathanLerner @Timcast Apparently the people g...</td>\n",
       "      <td>{'id': 1321166514188357633, 'id_str': '1321166...</td>\n",
       "      <td>Thu Dec 17 16:38:25 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217764</th>\n",
       "      <td>my uncle tried to compare taking a plane to ta...</td>\n",
       "      <td>{'id': 853405744514965504, 'id_str': '85340574...</td>\n",
       "      <td>Thu Dec 17 16:38:25 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217738</th>\n",
       "      <td>Africa is aiming to vaccinate up to 60% of its...</td>\n",
       "      <td>{'id': 141163282, 'id_str': '141163282', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:25 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217737</th>\n",
       "      <td>It is the responsibility of the media to provi...</td>\n",
       "      <td>{'id': 201255677, 'id_str': '201255677', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:25 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217730</th>\n",
       "      <td>I figured those doctors were getting shot with...</td>\n",
       "      <td>{'id': 2390453162, 'id_str': '2390453162', 'na...</td>\n",
       "      <td>Thu Dec 17 16:38:25 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217723</th>\n",
       "      <td>people get the flu vaccine and still get the f...</td>\n",
       "      <td>{'id': 578356651, 'id_str': '578356651', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:24 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217714</th>\n",
       "      <td>You can’t sue Pfizer or Moderna if you have se...</td>\n",
       "      <td>{'id': 1281297668577226752, 'id_str': '1281297...</td>\n",
       "      <td>Thu Dec 17 16:38:24 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217713</th>\n",
       "      <td>@realDonaldTrump US shatters Covid records for...</td>\n",
       "      <td>{'id': 500114262, 'id_str': '500114262', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:24 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217705</th>\n",
       "      <td>Great news: We may have 20-40% more doses of P...</td>\n",
       "      <td>{'id': 2735511486, 'id_str': '2735511486', 'na...</td>\n",
       "      <td>Thu Dec 17 16:38:24 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217666</th>\n",
       "      <td>What’s REALLY IN THE VACCINE😳🙄😢😡😤🤢🤮</td>\n",
       "      <td>{'id': 622298211, 'id_str': '622298211', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:23 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217652</th>\n",
       "      <td>I don’t think I’ll ever be able to wrap my hea...</td>\n",
       "      <td>{'id': 2729997030, 'id_str': '2729997030', 'na...</td>\n",
       "      <td>Thu Dec 17 16:38:23 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217642</th>\n",
       "      <td>So is the CDC or FDA recommending folks get an...</td>\n",
       "      <td>{'id': 882749520026771456, 'id_str': '88274952...</td>\n",
       "      <td>Thu Dec 17 16:38:22 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217633</th>\n",
       "      <td>@benci_cheri @sallyeastman1 @Lrihendry The Mod...</td>\n",
       "      <td>{'id': 350058027, 'id_str': '350058027', 'name...</td>\n",
       "      <td>Thu Dec 17 16:38:22 +0000 2020</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "217856  @CarrieeeeC They're having panic attacks over ...   \n",
       "217837  What if I told you we already have the vaccine...   \n",
       "217836                        The vaccine gonna be $600 😭   \n",
       "217831  We deserve royalties! We paid for the vaccine ...   \n",
       "217794  @realDonaldTrump People are dying because of t...   \n",
       "217783  Some vaccine doses kept too cold, Pfizer havin...   \n",
       "217770  Give me an update on the H1N1 VACCINE. Stand u...   \n",
       "217769  @NathanLerner @Timcast Apparently the people g...   \n",
       "217764  my uncle tried to compare taking a plane to ta...   \n",
       "217738  Africa is aiming to vaccinate up to 60% of its...   \n",
       "217737  It is the responsibility of the media to provi...   \n",
       "217730  I figured those doctors were getting shot with...   \n",
       "217723  people get the flu vaccine and still get the f...   \n",
       "217714  You can’t sue Pfizer or Moderna if you have se...   \n",
       "217713  @realDonaldTrump US shatters Covid records for...   \n",
       "217705  Great news: We may have 20-40% more doses of P...   \n",
       "217666                What’s REALLY IN THE VACCINE😳🙄😢😡😤🤢🤮   \n",
       "217652  I don’t think I’ll ever be able to wrap my hea...   \n",
       "217642  So is the CDC or FDA recommending folks get an...   \n",
       "217633  @benci_cheri @sallyeastman1 @Lrihendry The Mod...   \n",
       "\n",
       "                                                     user  \\\n",
       "217856  {'id': 23965688, 'id_str': '23965688', 'name':...   \n",
       "217837  {'id': 972294050232532997, 'id_str': '97229405...   \n",
       "217836  {'id': 3741790334, 'id_str': '3741790334', 'na...   \n",
       "217831  {'id': 1008411133458436096, 'id_str': '1008411...   \n",
       "217794  {'id': 1095880612634918912, 'id_str': '1095880...   \n",
       "217783  {'id': 360024367, 'id_str': '360024367', 'name...   \n",
       "217770  {'id': 278770541, 'id_str': '278770541', 'name...   \n",
       "217769  {'id': 1321166514188357633, 'id_str': '1321166...   \n",
       "217764  {'id': 853405744514965504, 'id_str': '85340574...   \n",
       "217738  {'id': 141163282, 'id_str': '141163282', 'name...   \n",
       "217737  {'id': 201255677, 'id_str': '201255677', 'name...   \n",
       "217730  {'id': 2390453162, 'id_str': '2390453162', 'na...   \n",
       "217723  {'id': 578356651, 'id_str': '578356651', 'name...   \n",
       "217714  {'id': 1281297668577226752, 'id_str': '1281297...   \n",
       "217713  {'id': 500114262, 'id_str': '500114262', 'name...   \n",
       "217705  {'id': 2735511486, 'id_str': '2735511486', 'na...   \n",
       "217666  {'id': 622298211, 'id_str': '622298211', 'name...   \n",
       "217652  {'id': 2729997030, 'id_str': '2729997030', 'na...   \n",
       "217642  {'id': 882749520026771456, 'id_str': '88274952...   \n",
       "217633  {'id': 350058027, 'id_str': '350058027', 'name...   \n",
       "\n",
       "                            created_at hashtags  favorite_count  \\\n",
       "217856  Thu Dec 17 16:38:28 +0000 2020     None               0   \n",
       "217837  Thu Dec 17 16:38:27 +0000 2020     None               0   \n",
       "217836  Thu Dec 17 16:38:27 +0000 2020     None               0   \n",
       "217831  Thu Dec 17 16:38:27 +0000 2020     None               0   \n",
       "217794  Thu Dec 17 16:38:26 +0000 2020     None               0   \n",
       "217783  Thu Dec 17 16:38:26 +0000 2020     None               0   \n",
       "217770  Thu Dec 17 16:38:25 +0000 2020     None               0   \n",
       "217769  Thu Dec 17 16:38:25 +0000 2020     None               0   \n",
       "217764  Thu Dec 17 16:38:25 +0000 2020     None               0   \n",
       "217738  Thu Dec 17 16:38:25 +0000 2020     None               0   \n",
       "217737  Thu Dec 17 16:38:25 +0000 2020     None               0   \n",
       "217730  Thu Dec 17 16:38:25 +0000 2020     None               0   \n",
       "217723  Thu Dec 17 16:38:24 +0000 2020     None               0   \n",
       "217714  Thu Dec 17 16:38:24 +0000 2020     None               0   \n",
       "217713  Thu Dec 17 16:38:24 +0000 2020     None               0   \n",
       "217705  Thu Dec 17 16:38:24 +0000 2020     None               0   \n",
       "217666  Thu Dec 17 16:38:23 +0000 2020     None               0   \n",
       "217652  Thu Dec 17 16:38:23 +0000 2020     None               0   \n",
       "217642  Thu Dec 17 16:38:22 +0000 2020     None               0   \n",
       "217633  Thu Dec 17 16:38:22 +0000 2020     None               0   \n",
       "\n",
       "        retweet_count                                             source  \n",
       "217856              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217837              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "217836              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217831              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217794              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217783              0  <a href=\"http://publicize.wp.com/\" rel=\"nofoll...  \n",
       "217770              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217769              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217764              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217738              0  <a href=\"https://about.twitter.com/products/tw...  \n",
       "217737              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217730              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217723              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217714              0  <a href=\"http://twitter.com/download/android\" ...  \n",
       "217713              0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...  \n",
       "217705              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217666              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217652              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217642              0  <a href=\"http://twitter.com/download/iphone\" r...  \n",
       "217633              0  <a href=\"http://twitter.com/download/iphone\" r...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "ranked_docs = search_tf_idf(query, index)    \n",
    "top = 1\n",
    "\n",
    "query = df_tweets.loc[ranked_docs[:top]]['text'].values\n",
    "ranked_docs = search_using_new_ranking(query[0], index)    \n",
    "top = 20\n",
    "\n",
    "df_tweets.loc[ranked_docs[:top]][['text', 'user', 'created_at', 'hashtags' , 'favorite_count', 'retweet_count', 'source']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
