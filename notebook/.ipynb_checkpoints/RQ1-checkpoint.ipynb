{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import Counter\n",
    "from config import *\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import unidecode\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk import bigrams\n",
    "import time\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"notebook/one.json\", \"rb\") as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [json.loads(str_) for str_ in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame.from_records(lines)\n",
    "df_tweets['h_field'] = df_tweets[\"entities\"].apply(lambda x: x['hashtags'])\n",
    "df_tweets['hashtags'] = df_tweets['h_field'].apply(lambda x: x[0]['text'] if x != [] else None)\n",
    "df_tweets['retweeted_status'] = df_tweets['retweeted_status'].apply(lambda x: 1 if str(x)== 'nan' else 0)\n",
    "df_tweets['id_tweet'] = df_tweets.index\n",
    "\n",
    "data = df_tweets[df_tweets['retweeted_status'] == 1][['id_tweet', 'text']]\n",
    "#data['id_tweet'] = str(data['id_tweet'])\n",
    "data['join'] = data[['id_tweet', 'text']].apply(lambda x: '{}|{}'.format(x[0],x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    if text:\n",
    "        return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    if text:\n",
    "        return re.sub(r'#\\w+ ?', '', text)\n",
    "    \n",
    "    return \"\"\n",
    "def remove_accents(text):\n",
    "    if text:\n",
    "        return unidecode.unidecode(text)\n",
    "        \n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_punctuation_marks(text):\n",
    "    if text:\n",
    "        translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        new_text = text.translate(translator)\n",
    "        return \" \".join(new_text.split())\n",
    "        \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def text_to_lower_case(text):\n",
    "    if text:\n",
    "        return text.lower()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_emojis(text):\n",
    "    if text:\n",
    "        # TODO: Remove emojis (tip: search for encode - decode)\n",
    "        returnString = \"\"\n",
    "        for character in text:\n",
    "            try:\n",
    "                character.encode(\"ascii\")\n",
    "                returnString += character\n",
    "            except UnicodeEncodeError:\n",
    "                returnString += ''\n",
    "        text = \" \".join(returnString.split())\n",
    "        \n",
    "    return text\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_multiple_whitespaces(text):\n",
    "    if text:\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def remove_text_marks(text):\n",
    "    if text:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # TODO: replace *, ?, ... by spaces\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    # In case there is no text\n",
    "    return \"\"\n",
    "\n",
    "def split_text_and_numbers(text):\n",
    "    temp = re.compile(\"([a-zA-Z]+)([0-9]+)\") \n",
    "    try:\n",
    "        res = temp.match(text).groups()\n",
    "        text = \" \".join(res)\n",
    "    except:\n",
    "        text = text\n",
    "    return text\n",
    "\n",
    "def remove_alone_numbers(text):\n",
    "    if text:\n",
    "        text = ' '.join(filter(lambda word:word.replace('.','').isdigit()==False, text.split()))\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = text.split()\n",
    "    return \" \".join([x for x in text if x not in STOPWORDS])\n",
    "\n",
    "def stemming(text):\n",
    "    text = text.split()\n",
    "    return \" \".join([ps.stem(x) for x in text])\n",
    "\n",
    "def clean_text(text):\n",
    "    # Apply the different functions in order to clean the text\n",
    "    text = remove_links(text)\n",
    "    text = remove_hashtags(text)\n",
    "    text = text_to_lower_case(text)\n",
    "    text = remove_text_marks(text)\n",
    "    text = remove_accents(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = split_text_and_numbers(text)\n",
    "    #text = remove_alone_numbers(text)\n",
    "    text = remove_multiple_whitespaces(text)\n",
    "    text = remove_punctuation_marks(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stemming(text)\n",
    "    \n",
    "    # Return\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = ['trump has covid', 'impact of coronavirus', 'how to face off covid',\n",
    "           'masks and lockdowns don\\'t work', 'how good is the vaccine?', 'vaccine of pfizer', \n",
    "          'vaccine 2021', 'no more masks', 'I think I have covid', 'what are covid symptoms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text'] = data['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "for line in data['clean_text']:\n",
    "    sent.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(sent, min_count = 30, progress_per= 10000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time.time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=clean_text('trump'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model=KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "for line in sent:\n",
    "    for i in line:\n",
    "        if i not in keys:\n",
    "            if i in w2v_model.wv.vocab:\n",
    "                keys.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in w2v_model.most_similar(word, topn=30):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(w2v_model[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=250, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#\n",
    "\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        #for i, word in enumerate(words):\n",
    "            #plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         #textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% matplotlib inline\n",
    "tsne_plot_similar_words('Similar words from Google News', keys, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
